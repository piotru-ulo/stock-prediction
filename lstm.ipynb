{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r3ivRENixxiu",
        "outputId": "f047e453-724f-4f20-b912-c611317d2af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Selecting best features...\n",
            "Selected 8 best features:\n",
            "  1. price_momentum: 0.221\n",
            "  2. ma_10: 0.894\n",
            "  3. ma_5: 0.931\n",
            "  4. close_lag_1: 0.935\n",
            "  5. open: 0.952\n",
            "  6. high: 0.959\n",
            "  7. low: 0.963\n",
            "  8. close: 0.967\n",
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.2186 - val_loss: 0.0920\n",
            "Epoch 2/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0888 - val_loss: 0.0167\n",
            "Epoch 3/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0508 - val_loss: 0.0233\n",
            "Epoch 4/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0480 - val_loss: 0.0513\n",
            "Epoch 5/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0473 - val_loss: 0.0225\n",
            "Epoch 6/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0348 - val_loss: 0.0061\n",
            "Epoch 7/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0275 - val_loss: 0.0074\n",
            "Epoch 8/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0226 - val_loss: 0.0062\n",
            "Epoch 9/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0203 - val_loss: 0.0074\n",
            "Epoch 10/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0155 - val_loss: 0.0080\n",
            "Epoch 11/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0181 - val_loss: 0.0072\n",
            "Epoch 12/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0211 - val_loss: 0.0071\n",
            "Epoch 13/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0168 - val_loss: 0.0067\n",
            "Epoch 14/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0148 - val_loss: 0.0065\n",
            "Epoch 15/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0175 - val_loss: 0.0063\n",
            "Epoch 16/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0183 - val_loss: 0.0077\n",
            "\n",
            "LSTM Model Performance Summary:\n",
            "Prediction Accuracy Metrics:\n",
            "Root Mean Squared Error:      10.7724\n",
            "Mean Absolute Error (MAE):    10.0591\n",
            "Mean Absolute Percentage Error: 5.74%\n",
            "Mean Squared Error (MSE):     116.0452\n",
            "Directional Accuracy:         55.56%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from stockdex import Ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from gaussian_proces import GaussianProcessStockPredictor\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_data(range, dataGranularity, stock):\n",
        "    ticker = Ticker(stock)\n",
        "    try:\n",
        "        data = ticker.yahoo_api_price(range=range, dataGranularity=dataGranularity)\n",
        "        df = pd.DataFrame({\n",
        "            'timestamp': data['timestamp'],\n",
        "            'open': data['open'],\n",
        "            'high': data['high'],\n",
        "            'low': data['low'],\n",
        "            'close': data['close'],\n",
        "            'volume': data['volume'],\n",
        "        })\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching data for {stock}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_sequences(data, target, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length])\n",
        "        y.append(target[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def add_enhanced_features(df):\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Lagged features\n",
        "    df_enhanced['close_lag_1'] = df_enhanced['close'].shift(1)\n",
        "    df_enhanced['return_1d'] = df_enhanced['close'].pct_change()\n",
        "\n",
        "    # Moving averages\n",
        "    df_enhanced['ma_5'] = df_enhanced['close'].rolling(window=5).mean()\n",
        "    df_enhanced['ma_10'] = df_enhanced['close'].rolling(window=10).mean()\n",
        "\n",
        "    # Momentum indicators\n",
        "    df_enhanced['price_momentum'] = (df_enhanced['close'] - df_enhanced['ma_5']) / df_enhanced['ma_5']\n",
        "\n",
        "    # Volume features\n",
        "    df_enhanced['volume_ma_5'] = df_enhanced['volume'].rolling(window=5).mean()\n",
        "    df_enhanced['volume_momentum'] = (df_enhanced['volume'] - df_enhanced['volume_ma_5']) / df_enhanced['volume_ma_5']\n",
        "\n",
        "    # Volatility\n",
        "    df_enhanced['intraday_vol'] = (df_enhanced['high'] - df_enhanced['low']) / df_enhanced['close']\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    df_enhanced = df_enhanced.dropna()\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def select_best_features(X, y, feature_names, k=8):\n",
        "    correlations = []\n",
        "\n",
        "    for i in range(X.shape[2]):\n",
        "        feature_data = X[:, -1, i]\n",
        "        correlation = np.corrcoef(feature_data, y.ravel())[0, 1]\n",
        "        correlations.append(abs(correlation))\n",
        "\n",
        "    k = min(k, len(feature_names))\n",
        "    top_k_indices = np.argsort(correlations)[-k:]\n",
        "\n",
        "    selected_feature_names = [feature_names[i] for i in top_k_indices]\n",
        "\n",
        "    print(f\"Selected {k} best features:\")\n",
        "    for i, name in enumerate(selected_feature_names):\n",
        "        idx = top_k_indices[i]\n",
        "        print(f\"  {i+1}. {name}: {correlations[idx]:.3f}\")\n",
        "\n",
        "    return X[:, :, top_k_indices], selected_feature_names\n",
        "\n",
        "def create_enhanced_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(32, return_sequences=False, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(X_train, y_train, X_test, y_test, close_scaler):\n",
        "    model = create_enhanced_model((X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "    predictions = close_scaler.inverse_transform(predictions)\n",
        "    y_test_actual = close_scaler.inverse_transform(y_test)\n",
        "\n",
        "    return predictions, y_test_actual, history\n",
        "\n",
        "def calculate_metrics(actual, predicted):\n",
        "    mse = mean_squared_error(actual, predicted)\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
        "\n",
        "    actual_direction = np.diff(actual.flatten()) > 0\n",
        "    predicted_direction = np.diff(predicted.flatten()) > 0\n",
        "    directional_accuracy = np.mean(actual_direction == predicted_direction) * 100\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MAPE': mape,\n",
        "        'Directional_Accuracy': directional_accuracy\n",
        "    }\n",
        "\n",
        "def plot_results(dates, actual, predicted, metrics):\n",
        "\n",
        "    # Plot actual and predicted prices with line and dots\n",
        "    plt.plot(dates, actual, label='Actual Price', linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "    plt.plot(dates, predicted, label='Predicted Price', linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "\n",
        "    plt.title('Price Prediction', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Close Price ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_performance_summary(metrics):\n",
        "\n",
        "\n",
        "    print(f\"Prediction Accuracy Metrics:\")\n",
        "    print(f\"Root Mean Squared Error:      {metrics['RMSE']:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE):    {metrics['MAE']:.4f}\")\n",
        "    print(f\"Mean Absolute Percentage Error: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"Mean Squared Error (MSE):     {metrics['MSE']:.4f}\")\n",
        "    print(f\"Directional Accuracy:         {metrics['Directional_Accuracy']:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = get_data('1y', '1d', 'GOOG')\n",
        "\n",
        "    if data is not None:\n",
        "\n",
        "        data_enhanced = add_enhanced_features(data)\n",
        "\n",
        "        feature_cols = [\n",
        "            'open', 'high', 'low', 'close', 'volume',\n",
        "            'close_lag_1', 'return_1d', 'ma_5', 'ma_10',\n",
        "            'price_momentum', 'volume_momentum', 'intraday_vol'\n",
        "        ]\n",
        "\n",
        "        feature_scaler = StandardScaler()\n",
        "        scaled_features = feature_scaler.fit_transform(data_enhanced[feature_cols])\n",
        "\n",
        "        close_scaler = MinMaxScaler()\n",
        "        scaled_close = close_scaler.fit_transform(data_enhanced[['close']])\n",
        "\n",
        "        sequence_length = 25\n",
        "        X, y = create_sequences(scaled_features, scaled_close, sequence_length)\n",
        "\n",
        "        print(f\"\\nSelecting best features...\")\n",
        "        X_selected, selected_features = select_best_features(X, y, feature_cols, k=8)\n",
        "\n",
        "        train_size = len(X_selected) - 10\n",
        "        X_train = X_selected[:train_size]\n",
        "        y_train = y[:train_size]\n",
        "        X_test = X_selected[train_size:]\n",
        "        y_test = y[train_size:]\n",
        "\n",
        "        predictions, actual, history = train_model(X_train, y_train, X_test, y_test, close_scaler)\n",
        "\n",
        "        metrics = calculate_metrics(actual, predictions)\n",
        "\n",
        "        dates = data_enhanced.index[sequence_length + train_size : sequence_length + train_size + len(predictions)]\n",
        "\n",
        "        print(\"\\nLSTM Model Performance Summary:\")\n",
        "        print_performance_summary(metrics)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(14, 6))\n",
        "        predictor = GaussianProcessStockPredictor(data)\n",
        "        dates, preds_gpr, stds = predictor.predict_n_days(10, normalize_y=True, n_restarts_optimizer=10)\n",
        "        preds_gpr = np.array(preds_gpr).reshape(-1, 1)  # Ensure it's a column vector\n",
        "\n",
        "        predictor.plot_predictions(dates, preds_gpr, stds, ax)\n",
        "\n",
        "        plot_results(dates, actual, predictions, metrics)\n",
        "\n",
        "        metrics_gpr = calculate_metrics(actual, preds_gpr)\n",
        "        print(\"\\nGaussian Process Regression Metrics:\")\n",
        "        print_performance_summary(metrics_gpr)\n",
        "       # avg_preds = (predictions.flatten()*0.8 + preds_gpr.flatten()*0.2)\n",
        "\n",
        "       # metrics_combined = calculate_metrics(actual, avg_preds)    \n",
        "   \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
